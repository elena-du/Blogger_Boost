{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "# Basics\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# gensim\n",
    "import gensim\n",
    "\n",
    "# keras\n",
    "np.random.seed(13)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Dense, Embedding, Reshape, Activation, \n",
    "                          SimpleRNN, LSTM, Convolution1D, \n",
    "                          MaxPooling1D, Dropout, Bidirectional)\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.datasets import imdb, reuters\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elena/opt/anaconda3/lib/python3.7/site-packages/nltk/decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import spacy\n",
    "import scattertext as st\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "#from spacy import displacy\n",
    "#from spacy.symbols import amod\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import swat\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/elena/Desktop/Metis/projects/5_project/Blogger_Boost/code\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../blogger_com_data_330677_7.pkl', 'rb') as picklefile:\n",
    "    df_toy = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>blogger_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>name</th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19,August,2004</td>\n",
       "      <td>4162441</td>\n",
       "      <td>male</td>\n",
       "      <td>16</td>\n",
       "      <td>Student</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>DESTINY...       you might not say a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19,August,2004</td>\n",
       "      <td>4157968</td>\n",
       "      <td>male</td>\n",
       "      <td>16</td>\n",
       "      <td>Student</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>As I strolled into the mall yesterday,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19,August,2004</td>\n",
       "      <td>4278694</td>\n",
       "      <td>female</td>\n",
       "      <td>24</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>I've never really had a blog bef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>19,August,2004</td>\n",
       "      <td>4261114</td>\n",
       "      <td>female</td>\n",
       "      <td>40</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>I love to read novels. I l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>19,August,2004</td>\n",
       "      <td>4215047</td>\n",
       "      <td>male</td>\n",
       "      <td>17</td>\n",
       "      <td>Student</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>today.. really a very sad day...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50463676</th>\n",
       "      <td>12,February,1999</td>\n",
       "      <td>2999514</td>\n",
       "      <td>male</td>\n",
       "      <td>27</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>[detail]           Propog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50482996</th>\n",
       "      <td>01,April,1999</td>\n",
       "      <td>2999514</td>\n",
       "      <td>male</td>\n",
       "      <td>27</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>[detail]         I actually...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50502316</th>\n",
       "      <td>05,June,1999</td>\n",
       "      <td>2999514</td>\n",
       "      <td>male</td>\n",
       "      <td>27</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>[detail]           A tria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50521636</th>\n",
       "      <td>16,December,1999</td>\n",
       "      <td>2999514</td>\n",
       "      <td>male</td>\n",
       "      <td>27</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>[detail]           Points...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50541018</th>\n",
       "      <td>23,August,2006</td>\n",
       "      <td>4246739</td>\n",
       "      <td>female</td>\n",
       "      <td>36</td>\n",
       "      <td>Arts</td>\n",
       "      <td>Scorpio</td>\n",
       "      <td>This is a picture of my mom ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>330677 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date blogger_id  gender age   occupation         name  \\\n",
       "0           19,August,2004    4162441    male  16      Student  Sagittarius   \n",
       "13          19,August,2004    4157968    male  16      Student       Pisces   \n",
       "17          19,August,2004    4278694  female  24   Technology        Virgo   \n",
       "69          19,August,2004    4261114  female  40       indUnk       Gemini   \n",
       "79          19,August,2004    4215047    male  17      Student     Aquarius   \n",
       "...                    ...        ...     ...  ..          ...          ...   \n",
       "50463676  12,February,1999    2999514    male  27  Engineering       Pisces   \n",
       "50482996     01,April,1999    2999514    male  27  Engineering       Pisces   \n",
       "50502316      05,June,1999    2999514    male  27  Engineering       Pisces   \n",
       "50521636  16,December,1999    2999514    male  27  Engineering       Pisces   \n",
       "50541018    23,August,2006    4246739  female  36         Arts      Scorpio   \n",
       "\n",
       "                                                       post  \n",
       "0                   DESTINY...       you might not say a...  \n",
       "13                As I strolled into the mall yesterday,...  \n",
       "17                      I've never really had a blog bef...  \n",
       "69                            I love to read novels. I l...  \n",
       "79                         today.. really a very sad day...  \n",
       "...                                                     ...  \n",
       "50463676                       [detail]           Propog...  \n",
       "50482996                     [detail]         I actually...  \n",
       "50502316                       [detail]           A tria...  \n",
       "50521636                       [detail]           Points...  \n",
       "50541018                    This is a picture of my mom ...  \n",
       "\n",
       "[330677 rows x 7 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"          DESTINY...       you might not say anything     but i can hear     you have chosen me, your life partner     so have i dear,     so have i dear....      my first dream, my first extreme,     my first love, i was waiting for my DESTINY.     what should i do with myself,     tell me o' my heart     what should i do with myself,     tell me....      should i fly, with this beautiful nature.     or should i play with these winds.     should i try to reach the skies,     or should i pray to the mother earth.     what should i do with myself friends     tell me....      she talked in such a way,     gave me dreams with thousand colours.     like i stand in the middle of island,     and she shows me all the love she has,     my first dream , my first extreme,     my first love, i was waiting for my DESTINY.           --NIL            \""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = df_toy['post'][0]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'That is not dead which can eternal lie, And with strange aeons death may die.\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in Corpus using Keras utility\n",
    "# We'll use some Lovecraft\n",
    "##!curl -o lovecraft.txt https://raw.githubusercontent.com/urschrei/lovecraft/master/lovecraft.txt\n",
    "\n",
    "corpus = open(\"lovecraft.txt\").readlines()[0:200]\n",
    "\n",
    "corpus[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [word for word in corpus if word.count(\" \") >= 1]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters=\"\"\"!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\"\"\")\n",
    "tokenizer.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8],\n",
       " [1],\n",
       " [6],\n",
       " [2],\n",
       " [4],\n",
       " [9],\n",
       " [10],\n",
       " [],\n",
       " [14],\n",
       " [4],\n",
       " [15],\n",
       " [11],\n",
       " [2],\n",
       " [],\n",
       " [6],\n",
       " [5],\n",
       " [10],\n",
       " [],\n",
       " [5],\n",
       " [9],\n",
       " [10],\n",
       " [2],\n",
       " [11],\n",
       " [4],\n",
       " [9],\n",
       " [15],\n",
       " [],\n",
       " [11],\n",
       " [1],\n",
       " [5],\n",
       " [3],\n",
       " [],\n",
       " [19],\n",
       " [11],\n",
       " [13],\n",
       " [6],\n",
       " [1],\n",
       " [9],\n",
       " [],\n",
       " [7],\n",
       " [4],\n",
       " [12],\n",
       " [1],\n",
       " [],\n",
       " [20],\n",
       " [5],\n",
       " [3],\n",
       " [2],\n",
       " [9],\n",
       " [1],\n",
       " [3],\n",
       " [],\n",
       " [8],\n",
       " [1],\n",
       " [5],\n",
       " [3],\n",
       " [],\n",
       " [8],\n",
       " [1],\n",
       " [5],\n",
       " [3],\n",
       " [],\n",
       " [12],\n",
       " [4],\n",
       " [3],\n",
       " [6],\n",
       " [2],\n",
       " [],\n",
       " [8],\n",
       " [3],\n",
       " [1],\n",
       " [5],\n",
       " [14],\n",
       " [],\n",
       " [12],\n",
       " [4],\n",
       " [3],\n",
       " [6],\n",
       " [2],\n",
       " [],\n",
       " [1],\n",
       " [22],\n",
       " [2],\n",
       " [3],\n",
       " [1],\n",
       " [14],\n",
       " [1],\n",
       " [],\n",
       " [12],\n",
       " [4],\n",
       " [3],\n",
       " [6],\n",
       " [2],\n",
       " [],\n",
       " [7],\n",
       " [13],\n",
       " [18],\n",
       " [1],\n",
       " [],\n",
       " [16],\n",
       " [5],\n",
       " [4],\n",
       " [2],\n",
       " [4],\n",
       " [9],\n",
       " [15],\n",
       " [],\n",
       " [8],\n",
       " [1],\n",
       " [6],\n",
       " [2],\n",
       " [4],\n",
       " [9],\n",
       " [10],\n",
       " [],\n",
       " [2],\n",
       " [1],\n",
       " [7],\n",
       " [7],\n",
       " [],\n",
       " [11],\n",
       " [1],\n",
       " [5],\n",
       " [3],\n",
       " [2],\n",
       " [],\n",
       " [2],\n",
       " [1],\n",
       " [7],\n",
       " [7],\n",
       " [],\n",
       " [12],\n",
       " [7],\n",
       " [10],\n",
       " [],\n",
       " [23],\n",
       " [1],\n",
       " [5],\n",
       " [17],\n",
       " [2],\n",
       " [4],\n",
       " [12],\n",
       " [17],\n",
       " [7],\n",
       " [],\n",
       " [9],\n",
       " [5],\n",
       " [2],\n",
       " [17],\n",
       " [3],\n",
       " [1],\n",
       " [],\n",
       " [20],\n",
       " [7],\n",
       " [5],\n",
       " [10],\n",
       " [],\n",
       " [16],\n",
       " [4],\n",
       " [9],\n",
       " [8],\n",
       " [6],\n",
       " [],\n",
       " [2],\n",
       " [3],\n",
       " [10],\n",
       " [],\n",
       " [3],\n",
       " [1],\n",
       " [5],\n",
       " [19],\n",
       " [11],\n",
       " [],\n",
       " [6],\n",
       " [21],\n",
       " [4],\n",
       " [1],\n",
       " [6],\n",
       " [],\n",
       " [20],\n",
       " [3],\n",
       " [5],\n",
       " [10],\n",
       " [],\n",
       " [14],\n",
       " [13],\n",
       " [2],\n",
       " [11],\n",
       " [1],\n",
       " [3],\n",
       " [],\n",
       " [1],\n",
       " [5],\n",
       " [3],\n",
       " [2],\n",
       " [11],\n",
       " [],\n",
       " [12],\n",
       " [3],\n",
       " [4],\n",
       " [1],\n",
       " [9],\n",
       " [8],\n",
       " [6],\n",
       " [],\n",
       " [2],\n",
       " [1],\n",
       " [7],\n",
       " [7],\n",
       " [],\n",
       " [2],\n",
       " [5],\n",
       " [7],\n",
       " [21],\n",
       " [1],\n",
       " [8],\n",
       " [],\n",
       " [16],\n",
       " [5],\n",
       " [10],\n",
       " [],\n",
       " [15],\n",
       " [5],\n",
       " [18],\n",
       " [1],\n",
       " [],\n",
       " [8],\n",
       " [3],\n",
       " [1],\n",
       " [5],\n",
       " [14],\n",
       " [6],\n",
       " [],\n",
       " [2],\n",
       " [11],\n",
       " [13],\n",
       " [17],\n",
       " [6],\n",
       " [5],\n",
       " [9],\n",
       " [8],\n",
       " [],\n",
       " [19],\n",
       " [13],\n",
       " [7],\n",
       " [13],\n",
       " [17],\n",
       " [3],\n",
       " [6],\n",
       " [],\n",
       " [7],\n",
       " [4],\n",
       " [21],\n",
       " [1],\n",
       " [],\n",
       " [6],\n",
       " [2],\n",
       " [5],\n",
       " [9],\n",
       " [8],\n",
       " [],\n",
       " [14],\n",
       " [4],\n",
       " [8],\n",
       " [8],\n",
       " [7],\n",
       " [1],\n",
       " [],\n",
       " [4],\n",
       " [6],\n",
       " [7],\n",
       " [5],\n",
       " [9],\n",
       " [8],\n",
       " [],\n",
       " [6],\n",
       " [11],\n",
       " [13],\n",
       " [16],\n",
       " [6],\n",
       " [],\n",
       " [7],\n",
       " [13],\n",
       " [18],\n",
       " [1],\n",
       " [],\n",
       " [12],\n",
       " [4],\n",
       " [3],\n",
       " [6],\n",
       " [2],\n",
       " [],\n",
       " [8],\n",
       " [3],\n",
       " [1],\n",
       " [5],\n",
       " [14],\n",
       " [],\n",
       " [12],\n",
       " [4],\n",
       " [3],\n",
       " [6],\n",
       " [2],\n",
       " [],\n",
       " [1],\n",
       " [22],\n",
       " [2],\n",
       " [3],\n",
       " [1],\n",
       " [14],\n",
       " [1],\n",
       " [],\n",
       " [12],\n",
       " [4],\n",
       " [3],\n",
       " [6],\n",
       " [2],\n",
       " [],\n",
       " [7],\n",
       " [13],\n",
       " [18],\n",
       " [1],\n",
       " [],\n",
       " [16],\n",
       " [5],\n",
       " [4],\n",
       " [2],\n",
       " [4],\n",
       " [9],\n",
       " [15],\n",
       " [],\n",
       " [8],\n",
       " [1],\n",
       " [6],\n",
       " [2],\n",
       " [4],\n",
       " [9],\n",
       " [10],\n",
       " [],\n",
       " [9],\n",
       " [4],\n",
       " [7]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-6dd2cb25c5d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mnb_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# For simplicity, one \"sentence\" per line \n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "# Tokenize using Keras\n",
    "tokenizer = Tokenizer(filters=\"\"\"!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\"\"\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "# Convert tokenized sentences to sequence format\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "\n",
    "print(corpus[3])\n",
    "print(sequences[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_samples = sum(len(s) for s in corpus)\n",
    "nb_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size:  3316\n"
     ]
    }
   ],
   "source": [
    "# Setting parameters for our model:\n",
    "\n",
    "# Vocab size\n",
    "V = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Dimension to reduce to (length of word embedding vectors)\n",
    "dim = 100\n",
    "window_size = 2\n",
    "\n",
    "print(\"vocabulary size: \", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the inputs and outputs for all windows\n",
    "def generate_data(sequences, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    # For each line (sentence)\n",
    "    for words in sequences:\n",
    "        L = len(words)\n",
    "        # Choose the target word\n",
    "        for index, word in enumerate(words):\n",
    "            # Create the window\n",
    "            s = index-window_size\n",
    "            e = index+window_size+1\n",
    "                    \n",
    "            in_words = []\n",
    "            labels = []\n",
    "            # Create the input/outputs for skipgrams\n",
    "            for i in range(s, e):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    in_words.append([word] )\n",
    "                    labels.append(words[i])\n",
    "\n",
    "            x = np.array(in_words,dtype=np.int32)\n",
    "            y = np_utils.to_categorical(labels, V)\n",
    "            yield (x, y)\n",
    "            \n",
    "# We'll call this later on, from within our train_skipgram_model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1, 100)            331600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3316)              334916    \n",
      "=================================================================\n",
      "Total params: 666,516\n",
      "Trainable params: 666,516\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the Keras model and view it \n",
    "skipgram = Sequential()\n",
    "skipgram.add(Embedding(input_dim=V, input_length=1, embeddings_initializer=\"glorot_uniform\", output_dim=dim))\n",
    "skipgram.add(Reshape((dim, )))\n",
    "skipgram.add(Dense(input_dim=dim, units=V, activation='softmax'))\n",
    "\n",
    "skipgram.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_skipgram_model(skipgram, sequences, window_size, dimension_size=V):\n",
    "    \"\"\"\n",
    "    skipgram: Keras model to train\n",
    "    sequences: list of lists of integers. \n",
    "               sequences[i][j] is the encoding of word j in document i\n",
    "    window_size: number of words in the window\n",
    "    dimension_size: integer. Size of the vector space for the word vectors.\n",
    "    \n",
    "    Note: this is slow to train. Took an 1hr 40min (no GPU) on 2016 Macbook Pro.\n",
    "    \"\"\"\n",
    "    # Note this cell took 1hr 40min on my machine (no GPU)\n",
    "    # Compile the Keras Model\n",
    "    skipgram.compile(loss='categorical_crossentropy', optimizer=\"adadelta\")\n",
    "\n",
    "    # Fit the Skipgrams\n",
    "    for iteration in range(10):\n",
    "        loss = 0.\n",
    "        for sequence, label in generate_data(sequences, window_size, dimension_size):\n",
    "            loss += skipgram.train_on_batch(sequence, label)\n",
    "\n",
    "        print(iteration, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_model_weights(skipgram, tokenizer, filename='vectors.txt'):\n",
    "    # Write the resulting vectors to a text file\n",
    "    with open(filename ,'w') as f:\n",
    "        f.write(f\"{V-1} {dim}\\n\")\n",
    "        vectors = skipgram.get_weights()[0]\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            line = f\"{word} \" + \" \".join([str(num) for num in vectors[i,:]]) + \"\\n\"\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elena/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 105299.83390712738\n",
      "1 99556.8950150013\n",
      "2 99391.85418915749\n",
      "3 99290.92662787437\n",
      "4 99150.98761761189\n",
      "5 99028.40557861328\n",
      "6 98942.17820227146\n",
      "7 98846.22163951397\n",
      "8 98756.67071413994\n",
      "9 98677.5851148367\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8e1296b302a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vectors.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_skipgram_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskipgram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mwrite_model_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskipgram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'token' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('vectors.txt'):\n",
    "    train_skipgram_model(skipgram, sequences, window_size, V)\n",
    "    write_model_weights(skipgram, token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vectors into word2vec and see how we did!\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)\n",
    "w2v.most_similar(positive=['white', 'rabbit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elena/opt/anaconda3/lib/python3.7/site-packages/keras/datasets/imdb.py:49: UserWarning: The `nb_words` argument in `load_data` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `load_data` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17465344/17464789 [==============================] - 2s 0us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "maxlen = 100  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 100  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "brnn_model = Sequential()\n",
    "brnn_model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "\n",
    "# Bidirectional LSTM!!!\n",
    "brnn_model.add(Bidirectional(LSTM(64)))\n",
    "brnn_model.add(Dropout(0.5))\n",
    "brnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "brnn_model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "brnn_model.fit(X_train, y_train,\n",
    "               batch_size=batch_size,\n",
    "               epochs=4,\n",
    "               validation_data=[X_test, y_test])\n",
    "\n",
    "score, acc = brnn_model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-753876be31a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'max_features' is not defined"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
